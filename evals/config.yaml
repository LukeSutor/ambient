# Evaluation Configuration
server:
  # Server executable and connection
  executable: "../app/sec/src-tauri/binaries/server-x86_64-pc-windows-msvc.exe"
  host: "localhost"
  port: 8080
  timeout: 120
  auto_start: true
  auto_stop: true
  
  # Health check settings
  health_check_timeout: 30
  startup_timeout: 60

# Model configuration
model:
  path: "C:/Users/Luke/AppData/Roaming/com.tauri.dev/models/llm/Qwen3-1.7B-Q4_K_M.gguf"
  context_size: 4096
  gpu_layers: 0

# Generation parameters (defaults for entire session)
generation:
  temperature: 0.1
  max_tokens: 1000
  top_p: 0.9
  top_k: 40

# Evaluation settings
evaluation:
  # Task detection evaluation
  task_detection:
    enabled: true
    data_dir: "./task-detection/data"
    output_file: "./results/task_detection_results.json"
    generate_ground_truth: false
    batch_size: 10
    
  # Visualization settings
  visualization:
    enabled: true
    output_dir: "./plots"
    auto_generate: true
    
  # General settings
  log_level: "INFO"
  results_dir: "./results"

# JSON Schema configuration
schemas:
  # Path to store schema files
  schema_dir: "./schemas"

# Parallel processing
parallel:
  max_concurrent_requests: 4
  request_timeout: 60

# Logging
logging:
  level: "INFO"
  file: "evals.log"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

# Results
results:
  output_dir: "results"
  save_individual_predictions: true
  save_aggregated_metrics: true
  generate_visualizations: true
